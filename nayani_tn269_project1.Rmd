---
title: "Project"
author: "tejeshwar"
date: "4/21/2020"
output:
  pdf_document: default
  html_document: default
---

```{r Setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.height = 5, fig.width = 9)
```

```{r Loading packages , message=FALSE, warning=FALSE}
library(rvest)
library(dplyr)
library(tidyverse)
library(zoo)
library(tidyr)
library(reshape)
library(boot)
library(splines)
```

## 1) Compare covid19 data from united states with Italy and Spain and also predict the number of cases and deaths based on data from Italy and Spain
##### a)The link https://github.com/nytimes/covid-19-data/blob/master/us-states.csv contains a breakdown of US cases and deaths of COVID-19 by state.  
  
-------
```{r Scraping USA data from nytimes, eval=FALSE, include=FALSE}
#Extracting the data from the URL and loading it in to a data frame
url <- "https://github.com/nytimes/covid-19-data/blob/master/us-states.csv"

scraping_data  <- url %>%
  read_html() %>%
  html_nodes("table") %>% 
  .[1] %>%
  html_table(fill = TRUE) %>%
  as.data.frame(stringsAsFactors = FALSE)

head(scraping_data)
```


```{r Cleaning and saving the tidy version of USA covid data, eval=FALSE, include=FALSE}
#Covid19 data of USA
scraping_data <- scraping_data %>% select(2:ncol(scraping_data))

#Change the column names of the dataframe
colnames(scraping_data) <- scraping_data[1,]
scraping_data <- scraping_data %>% slice(2:n())

str(scraping_data)
#Change columns "cases" & "deaths" to numeric
scraping_data$cases <- as.numeric(scraping_data$cases)
scraping_data$deaths <- as.numeric(scraping_data$deaths)
str(scraping_data)

head(scraping_data)
print("usa_data has daily values for each state and we need to convert it to daily values for united states")

usa_data <- scraping_data %>% 
  select(-c("state","fips")) %>% 
  group_by(date) %>%
  summarise(cases = sum(cases, na.rm = TRUE), deaths = sum(deaths, na.rm = TRUE))

usa_state_data <- scraping_data %>% 
  select(-c("fips"))

#Saving the tidy version of scraped usa covid19 data
# write.csv(usa_data,"usa_data.csv", row.names = FALSE)
# write.csv(usa_state_data,"usa_state_data.csv", row.names = FALSE)
```
  
I have used an ongoing repository of data (GitHub) on coronavirus cases and deaths in the U.S, which is maintained by New York Times. In the repository, we have data on cumulative coronavirus cases and deaths can be found in three files, one for each of these geographic levels: U.S., states, and counties. I have scraped the states level of data from the GitHub since it offers more detail than country level of data. This was little bit straightforward since we have a csv file in the repository.  

```{r Analysis of US covid19 data, echo=FALSE}
usa_data <- read.csv("usa_data.csv",stringsAsFactors = FALSE)
usa_data$date <- as.Date(usa_data$date)
usa_data <- usa_data %>% 
  mutate(daily.cases = (cases - lag(cases)), daily.deaths = (deaths - lag(deaths)))
usa_data[is.na.data.frame(usa_data)] <- 0
```

The scraped data has all its columns as character types. Column data types are changed accordingly i.e., cases column changed to numeric and date column to date format. Since the data is at state level, I have grouped by date and added to get national level data for US. I have stored this tidy version of national data as “usa_data.csv” and state level data as “usa_state_level_data”.  

```{r include=FALSE}
usa_state_data <- read.csv("usa_state_data.csv",stringsAsFactors = FALSE)
usa_state_data$date <- as.Date(usa_state_data$date)
```

```{r}
head(usa_data)
head(usa_state_data)
```

```{r include=FALSE}
#US state wise total number of cases and deaths
usa_state_data_total <- usa_state_data %>% group_by(state) %>% summarise(cases=sum(cases),deaths=sum(deaths)) %>% arrange(desc(cases))
```

Top 5 states based on total number of cases
```{r}
head(usa_state_data_total,5)
```
Top 5 states based on total number of deaths
```{r}
usa_state_data_total %>% arrange(desc(deaths)) %>% head(5)
```

```{r include=FALSE}
#Extracting and adding daily values from cumulative values for cases & deaths
usa_state_data <- usa_state_data %>%
    group_by(state) %>%
    arrange(date) %>%
    mutate(daily.cases = (cases - lag(cases)), daily.deaths = (deaths - lag(deaths)))
usa_state_data[is.na.data.frame(usa_state_data)] <- 0
tail(usa_state_data)

#top 5 highest daily increases in cases and deaths
usa_state_data %>% arrange(desc(daily.cases)) %>% head(5)
usa_state_data %>% arrange(desc(daily.deaths)) %>% head(5)
```

All the top 5 highest daily increases in cases and deaths scenarios are from New York state, so lets look at top 5 state highest daily increases in cases and deaths

```{r echo=FALSE}
#top 5 state highest daily increases in cases and deaths
usa_state_data %>% select(state,daily.cases) %>% arrange(desc(daily.cases)) %>% 
  group_by(state) %>% top_n(1,daily.cases) %>% head(5)
usa_state_data %>% select(state,daily.deaths) %>% arrange(desc(daily.deaths)) %>% 
  group_by(state) %>% top_n(1,daily.deaths) %>% head(5)
```

“New York”, “New Jersey”, “Massachusetts”, “Michigan”, “California” and “Illinois” are the most effected states in the US. Let's plot the number of cases and deaths of these most affected states to get a better a idea of their situation.

```{r Plots based on US covid19 data, echo=FALSE, message=FALSE}
most.affected.states <- c("New York","New Jersey","Massachusetts","Michigan","California","Illinois")

df <- usa_state_data[usa_state_data$state %in% most.affected.states,]
# df <- spread(df,state,cases)
# df[is.na.data.frame(df)]<-0

# ggplot(data = df, mapping = aes(x = date, y = cases, color = state)) + geom_smooth()
# ggplot(data = df, mapping = aes(x = date, y = log(cases), color = state)) + geom_smooth()

ggplot(data = df, mapping = aes(x = date, y = daily.cases, color = state)) + geom_smooth()

# ggplot(data = df, mapping = aes(x = date, y = deaths, color = state)) + geom_smooth()
# ggplot(data = df, mapping = aes(x = date, y = log(deaths), color = state)) + geom_smooth()

ggplot(data = df, mapping = aes(x = date, y = daily.deaths, color = state)) + geom_smooth()
```
We can see very clearly that in states like New York, New jesrey, the number of cases and deaths have peaked and are in a downward trend, which is a good news. But in other states trend is upwards and it might take few weeks to see a peak and a decreasing trend.  


##### b)The link https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Italy contains a breakdown of Italy cases and deaths of COVID-19 & The link https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Spain contains a breakdown of Spain cases and deaths.    
  ------------
  
```{r Scraping italy data from wikipedia, eval=FALSE, include=FALSE}
url <- "https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Italy"

scraping_data  <- url %>%
  read_html() %>%
  html_nodes("body #content #bodyContent #mw-content-text .mw-parser-output table .mw-collapsible")

#First 16 html_nodes and last 7 doesn't have the data needed for us.
scraping_data <- scraping_data[17:(length(scraping_data)-7)]
  
data_list <- list()

for (i in 1:(length(scraping_data))){
  data <- scraping_data[[i]] %>% html_nodes("td") %>% html_text()
  date <- as.Date(data[1])
  cases <- as.numeric(gsub(",","",strsplit(data[3],"\\(")[[1]][1]))
  deaths <- as.numeric(gsub(",","",strsplit(data[4],"\\(")[[1]][1]))
  data_list[[i]] <- data.frame(date,cases,deaths)
}

italy_data <- do.call(rbind,data_list)
str(italy_data)

#Saving the tidy version of scraped Italy covid19 data
# write.csv(italy_data,"italy_data.csv", row.names = FALSE)
```

```{r Scraping spain data from wikipedia, eval=FALSE, include=FALSE}
url <- "https://en.wikipedia.org/wiki/2020_coronavirus_pandemic_in_Spain"

scraping_data  <- url %>%
  read_html() %>%
  html_nodes("body #content #bodyContent #mw-content-text .mw-parser-output table .mw-collapsible")

#First 16 html_nodes and last 7 doesn't have the data needed for us.
scraping_data <- scraping_data[19:(length(scraping_data)-8)]
  
data_list <- list()

for (i in 1:(length(scraping_data))){
  data <- scraping_data[[i]] %>% html_nodes("td") %>% html_text()
  date <- as.Date(data[1])
  cases <- as.numeric(gsub(",","",strsplit(data[3],"\\(")[[1]][1]))
  deaths <- as.numeric(gsub(",","",strsplit(data[4],"\\(")[[1]][1]))
  data_list[[i]] <- data.frame(date,cases,deaths)
}

spain_data <- do.call(rbind,data_list)
str(spain_data)
#Saving the tidy version of scraped Spain covid19 data
# write.csv(spain_data,"spain_data.csv", row.names = FALSE)
```

```{r Data analysis of italy and spain data, echo=FALSE}
italy_data <- read.csv("italy_data.csv",stringsAsFactors = FALSE)
italy_data$date <- as.Date(italy_data$date)
spain_data <- read.csv("spain_data.csv",stringsAsFactors = FALSE)
spain_data$date <- as.Date(spain_data$date)

italy_data <- italy_data %>% 
  mutate(daily.cases = (cases - lag(cases)), daily.deaths = (deaths - lag(deaths)))
italy_data[is.na.data.frame(italy_data)] <- 0

spain_data <- spain_data %>% 
  mutate(daily.cases = (cases - lag(cases)), daily.deaths = (deaths - lag(deaths)))
spain_data[is.na.data.frame(spain_data)] <- 0
```
  
Italy and Spain data: Wikipedia webpages “COVID-19 pandemic in Italy” and “COVID-19 pandemic in Spain” has the data for covid19 cases & deaths for Italy and Spain, respectively. I have found scraping data from Wikipedia to be difficult since here the data is not in a tabular from. 

```{r echo=FALSE}
sprintf("Highest number of daily cases in italy and spain are %d and %d respectively",max(italy_data$daily.cases), max(spain_data$daily.cases))

sprintf("Highest number of daily deaths in italy and spain are %d and %d respectively",max(italy_data$daily.deaths), max(spain_data$daily.deaths))
```

##### c) Comparing the covid19 data of US, Spain and Italy.  

-----------------

Let us look at the trend of covid19 data of US, Spain and Italy.

```{r Comparing covid19 data of US, Spain and Italy., echo=FALSE, message=FALSE, warning=FALSE}
df <- rbind(usa_data %>% mutate(country = "US"), spain_data %>% mutate(country = "spain"), italy_data %>% mutate(country = "italy"))

ggplot(data = df, mapping = aes(x = date, y = daily.cases, color = country)) + geom_smooth()
# ggplot(data = df, mapping = aes(x = date, y = daily.deaths, color = country)) + geom_smooth()
ggplot(data = df, mapping = aes(x = date, y = cases, color = country)) + geom_smooth()
# ggplot(data = df, mapping = aes(x = date, y = deaths, color = country)) + geom_smooth()
```

Initially when I proposed to compare italy, spain covid19 data to US, they numbers where similar but since then spain and italy had reached peak and their trend is now downwards. Whereas the US numbers still keep rising and looks like it reached peak recently.

But New York and New Jersey numbers are similar to ltaly and spain. Lets plot their graph and check it. 

```{r Comparing covid19 data of New York, New Jersey, Spain and Italy, echo=FALSE, message=FALSE, warning=FALSE}
df <- usa_state_data[usa_state_data$state%in%c("New York","New Jersey"),]
df <- bind_rows(df, spain_data %>% mutate(state = "spain"), italy_data %>% mutate(state = "italy"))

ggplot(data = df, mapping = aes(x = date, y = daily.cases, color = state)) + geom_smooth()
# ggplot(data = df, mapping = aes(x = date, y = daily.deaths, color = state)) + geom_smooth()
ggplot(data = df, mapping = aes(x = date, y = cases, color = state)) + geom_smooth()
# ggplot(data = df, mapping = aes(x = date, y = deaths, color = state)) + geom_smooth()
```

Here we can observe a better similarity in trends of number of cases. In all the cases peak has reached and downward trend has started.
  
I have mentioned in proposal I will predict number of US cases on Spain and Italy data but since then they have drastically changed. So now instead of what I proposed, I will try to fit a polynomial regression to New York data and predict their future values.

I used cross-validation to select the optimal degree for the polynomial  

```{r Builiding a regression on New York data, message=FALSE, warning=FALSE}
ny_data <- usa_state_data[usa_state_data$state=="New York",]
ny_data$day <- as.numeric(ny_data$date - min(ny_data$date) + 1)

mse <- rep(NA, 10)
for (i in 1:10) {
    fit <- glm(daily.cases ~ poly(day, i), data = ny_data)
    mse[i] <- cv.glm(ny_data, fit, K = 10)$delta[1]
}
plot(1:10, mse, xlab = "Degree", ylab = "Test MSE", type = "o" , main="Test MSE w.r.t degree of the fitted polynomial")
```
I have plotted test Mean squared error for each degree of polynomial. It shows that the CV error reduces as we increase degree from 1 to 4, stay same till degree 5, and then the starts increasing for higher degrees. We pick the polynomial degree as 4.

```{r echo=FALSE, message=FALSE, warning=FALSE}
lm.ny <- lm(daily.cases ~ poly(day,4),data=ny_data)
# summary(lm.ny)

new.data <- data.frame(day = 1:92)
new.data$daily.cases <- predict(lm.ny, new.data)
ggplot(new.data, mapping = aes(x = day, y = daily.cases, color = "r")) + geom_smooth() +
  geom_smooth(ny_data, mapping = aes(x = day, y = daily.cases, color = "b"))
```

We can clearly see that though a polynomial regression fits well, it doesn't predict it well(Expecting a downward trend). Let's build a regression spline and see if it's predicts better.  
  
I used cross-validation to select the optimal degree for freedom of B-splines.  

```{r echo=FALSE, message=FALSE, warning=FALSE}
cv <- rep(NA, 16)
for (i in 3:16) {
    fit <- glm(daily.cases ~ bs(day, df = i), data = ny_data)
    cv[i] <- cv.glm(ny_data, fit, K = 10)$delta[1]
}
plot(3:16, cv[-c(1, 2)], lwd = 2, xlab = "df", ylab = "CV error", type = "o", main="CV error w.r.t degree of freedom")
```

There is no visible trend in the plot, but CV error attains minimum at df=6, so we can choose 6 as the optimal degrees of freedom. We need to split day(1 to 64) into 5 parts to attain dof of 6. Using the ggplot of newyork daily cases, we can pick them as (4, 11, 30, 35, 42)

```{r message=FALSE, warning=FALSE}
bs.ny <- lm(daily.cases ~ bs(day, knots = c(4, 11, 30, 35, 42)),ny_data)

new.data <- data.frame(day = 1:92)
new.data$daily.cases <- predict(bs.ny, new.data)
ggplot(new.data, mapping = aes(x = day, y = daily.cases, color = "r")) + geom_smooth() +
  geom_smooth(ny_data, mapping = aes(x = day, y = daily.cases, color = "b"))
```

B-spline polynomial spline fits the data well and also predicts it better that the polynomial regression. This is an expected performance. 

**********

## 2) The effect of number of coronavirus cases on the economy by looking at S&P 500 stock market index.

##### a)The link https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC contains daily values of S&P index.

********* 
I have tried using world trading data api but it was not supported in R. So as I suggested in proposal used yahoo finance website instead to get the S&P 500 index.
```{r Scraping S&P 500 index data from yahoo finance, include=FALSE}
url <- "https://finance.yahoo.com/quote/%5EGSPC/history?p=%5EGSPC"

#S&P500 index - stockmarket index data
smindex_data  <- url %>%
  read_html() %>% 
  html_table() %>%
  .[[1]] %>%
  as.data.frame()

str(smindex_data)
```

```{r Cleaning and saving a tidy version of S&P 500 index data, include=FALSE}
#removing the last row since it doesnot contain data and is not required
smindex_data <- smindex_data %>% top_n(nrow(smindex_data)-1)

#Changing column names from Date to date and Close* to close
colnames(smindex_data)[1] <- "date"
colnames(smindex_data)[5] <- "close"

#Selecting only date, Open, high, low and close columns
smindex_data <- smindex_data[,c(1:5)]

#Remove commas from values before converting them to numeric
cl <- c("Open","High","Low","close") 
smindex_data[cl] <- lapply(smindex_data[cl], gsub, pattern = ",", replacement = "")

#convert all cloumns except date to numeric
smindex_data[, 2:5] <- sapply(smindex_data[, 2:5], as.numeric)
```


```{r}
str(smindex_data)
```
But the date column in the scraped data is a different format so had to modify it before converting it to a date type column. I have used month.abb vector to do so.
```{r}
month.abb
```

```{r include=FALSE}
#We need to change the date column to date format. Extracting the month and date from the date column to do so.
date <- strsplit(smindex_data$date,",")
date <- lapply(date, `[[`, 1)

for (i in 1:length(date)){
  #Get month number using month.abb which has month abbreviations
  month <- match(substr(date[[i]],1,3),month.abb)
  day <- as.numeric(substr(date[[i]],5,6))
  #Get new date using month, day and year
  smindex_data$date[i] <- as.character(as.Date(ISOdate(year = 2020, month = month, day = day)))
}

smindex_data$date <- as.Date(smindex_data$date)
```

```{r}
str(smindex_data)
# saving the tidy version 
# write.csv(smindex_data,"smiindex.csv",row.names = FALSE)
```
After modifying the date column, a tidy version is saved and is used in further analysis.

```{r echo=FALSE}
smindex_data <- read.csv("smiindex.csv",stringsAsFactors = FALSE)
smindex_data$date <- as.Date(smindex_data$date)
sprintf("The highest S&P index rose to is:%.2f and the lowest value it reached is:%.2f", max(smindex_data$High), min(smindex_data$Low))
```

With in a span of a month, from 21-Feb to 21-March, S&P index dropped from 3300 to 2200 points causing a panic. But since then it recovered and now it is around 2800.

```{r echo=FALSE}
smindex_data$change <- smindex_data$close - smindex_data$Open
sprintf("The highest gain in S&P index in a day is:%.2f and the highest drop is:%.2f", max(smindex_data$change), abs(min(smindex_data$change)))
```

##### b)Data analysis on US covid19 and S&P 500 data

------------------------------------------------

In order to find the effect of covid19 on S&P index, first we need to merge both the dataframes.I have used only Open and close index values while merging since only they were relevant. But merging will generate few NA values in the Open and Close columns since we don't have S&P index on few days(Weekends and public holidays).
```{r Data analysis on US covid19 and S&P 500 data, include=FALSE}
#combine both US covid19 data and S&P 500 on date
df <- merge(usa_data,smindex_data[,c(1,2,5)],by="date",all.x = TRUE)
str(df)
```
```{r echo=FALSE}
head(df)
```
So I have used na.locf function which replaced each NA with the most recent non-NA prior to it.
```{r}
df$Open <- na.locf(df$Open)
df$close <- na.locf(df$close)
head(df)
```

let's look at the corelation between the cases, deaths, S&P index open and close values.
```{r}
cor(df[,c("cases","deaths","Open","close")])
```

Cases and deaths both are negatively correlated with Open and Close. It tells us that increase in cases/deaths has had a negative effect on S&P index.

```{r message=FALSE, warning=FALSE}
ggplot(df, aes(x = df$date)) +
  geom_smooth(aes(y = scale(df$cases), colour = "covid19 cases")) +
  geom_smooth(aes(y = scale(df$deaths), colour = "covid19 deaths")) +
  geom_smooth(aes(y = scale(df$daily.cases), colour = "covid19 daily cases")) +
  geom_smooth(aes(y = scale(df$daily.deaths), colour = "covid19 daily deaths")) +
  geom_smooth(aes(y = scale(df$Open), colour = "S&P daily opening value")) +
  geom_smooth(aes(y = scale(df$close), colour = "S&P daily closing value")) +
  xlab("predicted probit probability") + ylab("")
```
Though these we don't observe a clear negative trend but we can see that S&P index intially dropped due to rise in covid19 cases but after reaching a low has started to rise. That might due to increased measures taken by govt or other reasons, which might have made the public to reassure about US economy.
  